{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "529c9f8759bede575fc3c1c0a7d57d5ffc5b30205a4d6a007108b8ded7bffe54"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment:\n",
    "    NEGATIVE = 'NEGATIVE'\n",
    "    NEUTRAL = 'NEUTRAL'\n",
    "    POSITIVE = 'POSITIVE'\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, text, score):\n",
    "        self.text = text\n",
    "        self.score = score\n",
    "        self.sentiment = self.get_sentiment()\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        if self.score <=2:\n",
    "            return Sentiment.NEGATIVE\n",
    "        elif self.score == 3:\n",
    "            return Sentiment.NEUTRAL\n",
    "        else: \n",
    "            return Sentiment.POSITIVE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_name = 'data/sentiment/books_small.json'\n",
    "reviews = []\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line)\n",
    "        reviews.append(Review(review['reviewText'], review['overall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Very simple book, but leaves you feeling good.  No over the top sex scenes, no graphic violence.  Just a simple book that talks touches on child abuse and dysfunction in a family.  I liked it because the author did not go into graphic detail about the abuse, but instead focused how it affected the people involved.  Good story, good characters.'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "reviews[2].text"
   ]
  },
  {
   "source": [
    "### Particionar en prueba y entrenamiento"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(reviews, test_size=0.33, random_state=42)"
   ]
  },
  {
   "source": [
    "### Descomponer entre datos y etiquetas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [elem.text for elem in train]\n",
    "train_y = [elem.sentiment for elem in train]\n",
    "\n",
    "test_x = [elem.text for elem in test]\n",
    "test_y = [elem.sentiment for elem in test]"
   ]
  },
  {
   "source": [
    "### Utilizar vectorizer para la tokenization sobre los datos de tipo texto\n",
    "#####\n",
    "La tokenización es el proceso de dividir el texto en un conjunto de piezas significativas. Estas piezas se llaman tokens. Permite utilizar el principio de bag of words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<1x7372 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_x_vector = vectorizer.fit_transform(train_x)\n",
    "test_x_vector = vectorizer.transform(test_x)\n",
    "\n",
    "train_x_vector[0]"
   ]
  },
  {
   "source": [
    "## Clasificadores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### SVM\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "clf_svm.fit(train_x_vector, train_y)\n",
    "\n",
    "clf_svm.predict(test_x_vector[0])"
   ]
  },
  {
   "source": [
    "### Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_tree = DecisionTreeClassifier()\n",
    "clf_tree.fit(train_x_vector, train_y)\n",
    "\n",
    "clf_tree.predict(test_x_vector[0])"
   ]
  },
  {
   "source": [
    "### Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_nb = GaussianNB()\n",
    "clf_nb.fit(train_x_vector.toarray(), train_y)\n",
    "\n",
    "clf_nb.predict(test_x_vector[0].toarray())"
   ]
  },
  {
   "source": [
    "### Logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_lor = LogisticRegression()\n",
    "clf_lor.fit(train_x_vector, train_y)\n",
    "\n",
    "clf_lor.predict(test_x_vector[0])"
   ]
  },
  {
   "source": [
    "## Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "El score para la svm es de: 0.8242424242424242\nEl score para el decision tree es de: 0.7666666666666667\nEl score para la naive bayes es de: 0.8121212121212121\nEl score para la logistic regression es de: 0.8303030303030303\n"
     ]
    }
   ],
   "source": [
    "# Mean accuracy\n",
    "\n",
    "print('El score para la svm es de: {}'.format(clf_svm.score(test_x_vector, test_y)))\n",
    "print('El score para el decision tree es de: {}'.format(clf_tree.score(test_x_vector, test_y)))\n",
    "print('El score para la naive bayes es de: {}'.format(clf_nb.score(test_x_vector.toarray(), test_y)))\n",
    "print('El score para la logistic regression es de: {}'.format(clf_lor.score(test_x_vector, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "El f1 score para la svm es de: [0.91319444 0.21052632 0.22222222]\nEl f1 score para el decision tree es de: [0.87017544 0.13333333 0.06666667]\nEl f1 score para la naive bayes es de: [0.89678511 0.08510638 0.09090909]\nEl f1 score para la logistic regression es de: [0.91370558 0.12244898 0.1       ]\n"
     ]
    }
   ],
   "source": [
    "# F1 scores\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "score_svm = f1_score(test_y, clf_svm.predict(test_x_vector), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEUTRAL, Sentiment.NEGATIVE])\n",
    "score_dt = f1_score(test_y, clf_tree.predict(test_x_vector), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEUTRAL, Sentiment.NEGATIVE])\n",
    "score_nb = f1_score(test_y, clf_nb.predict(test_x_vector.toarray()), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEUTRAL, Sentiment.NEGATIVE])\n",
    "score_lor = f1_score(test_y, clf_lor.predict(test_x_vector), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEUTRAL, Sentiment.NEGATIVE])\n",
    "\n",
    "print('El f1 score para la svm es de: {}'.format(score_svm))\n",
    "print('El f1 score para el decision tree es de: {}'.format(score_dt))\n",
    "print('El f1 score para la naive bayes es de: {}'.format(score_nb))\n",
    "print('El f1 score para la logistic regression es de: {}'.format(score_lor))"
   ]
  },
  {
   "source": [
    "#### Un f1 score tan bajo puede ser debido a la distribucion de los conjuntos de datos\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReviewContainer:\n",
    "    \"\"\"Función para crear una distribución normal entre los conjuntos de datos\"\"\"\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "        self.evenly_distributed()\n",
    "    \n",
    "    def get_text(self):\n",
    "        return [review.text for review in self.reviews]\n",
    "\n",
    "    def get_sentiment(self):        \n",
    "        return [review.sentiment for review in self.reviews]\n",
    "\n",
    "    def evenly_distributed(self):\n",
    "        positive = list(filter(lambda x: x.sentiment == Sentiment.POSITIVE, self.reviews))\n",
    "        negative = list(filter(lambda x: x.sentiment == Sentiment.NEGATIVE, self.reviews))\n",
    "        self.reviews = positive[:len(negative)] + negative\n",
    "        random.shuffle(self.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_container = ReviewContainer(train)\n",
    "test_container = ReviewContainer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "new_train_x = train_container.get_text()\n",
    "new_train_y = train_container.get_sentiment()\n",
    "\n",
    "new_test_x = test_container.get_text()\n",
    "new_test_y = test_container.get_sentiment()\n",
    "\n",
    "print(new_train_y.count(Sentiment.NEUTRAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "new_train_y.count(Sentiment.NEGATIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<1x8906 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 66 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "new_train_x_vector = vectorizer.fit_transform(new_train_x)\n",
    "new_test_x_vector = vectorizer.transform(new_test_x)\n",
    "\n",
    "new_train_x_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "new_clf_svm = svm.SVC(kernel='linear')\n",
    "new_clf_svm.fit(new_train_x_vector, new_train_y)\n",
    "\n",
    "new_clf_svm.predict(new_test_x_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "new_clf_tree = DecisionTreeClassifier()\n",
    "new_clf_tree.fit(new_train_x_vector, new_train_y)\n",
    "\n",
    "new_clf_tree.predict(new_test_x_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "new_clf_nb = GaussianNB()\n",
    "new_clf_nb.fit(new_train_x_vector.toarray(), new_train_y)\n",
    "\n",
    "new_clf_nb.predict(new_test_x_vector[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "new_clf_lor = LogisticRegression()\n",
    "new_clf_lor.fit(new_train_x_vector, new_train_y)\n",
    "\n",
    "new_clf_lor.predict(new_test_x_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "El score para la svm es de: 0.7980769230769231\nEl score para el decision tree es de: 0.6298076923076923\nEl score para la naive bayes es de: 0.6346153846153846\nEl score para la logistic regression es de: 0.8149038461538461\n"
     ]
    }
   ],
   "source": [
    "# Mean accuracy\n",
    "\n",
    "print('El score para la svm es de: {}'.format(new_clf_svm.score(new_test_x_vector, new_test_y)))\n",
    "print('El score para el decision tree es de: {}'.format(new_clf_tree.score(new_test_x_vector, new_test_y)))\n",
    "print('El score para la naive bayes es de: {}'.format(new_clf_nb.score(new_test_x_vector.toarray(), new_test_y)))\n",
    "print('El score para la logistic regression es de: {}'.format(new_clf_lor.score(new_test_x_vector, new_test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "El f1 score para la svm es de: [0.8028169  0.         0.79310345]\nEl f1 score para el decision tree es de: [0.64018692 0.         0.61881188]\nEl f1 score para la naive bayes es de: [0.59574468 0.         0.66666667]\nEl f1 score para la logistic regression es de: [0.82051282 0.         0.808933  ]\n"
     ]
    }
   ],
   "source": [
    "# F1 scores\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "new_score_svm = f1_score(new_test_y, new_clf_svm.predict(new_test_x_vector), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEUTRAL, Sentiment.NEGATIVE])\n",
    "new_score_dt = f1_score(new_test_y, new_clf_tree.predict(new_test_x_vector), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEUTRAL, Sentiment.NEGATIVE])\n",
    "new_score_nb = f1_score(new_test_y, new_clf_nb.predict(new_test_x_vector.toarray()), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEUTRAL, Sentiment.NEGATIVE])\n",
    "new_score_lor = f1_score(new_test_y, new_clf_lor.predict(new_test_x_vector), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEUTRAL, Sentiment.NEGATIVE])\n",
    "\n",
    "print('El f1 score para la svm es de: {}'.format(new_score_svm))\n",
    "print('El f1 score para el decision tree es de: {}'.format(new_score_dt))\n",
    "print('El f1 score para la naive bayes es de: {}'.format(new_score_nb))\n",
    "print('El f1 score para la logistic regression es de: {}'.format(new_score_lor))"
   ]
  },
  {
   "source": [
    "### Un f1 score tan bajo puede ser debido a la poca cantidad de datos \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_name = 'data/sentiment/books_small_10000.json'\n",
    "reviews = []\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line)\n",
    "        reviews.append(Review(review['reviewText'], review['overall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(reviews, test_size=0.33, random_state=42)"
   ]
  },
  {
   "source": [
    "## Ensayo con data toy\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['POSITIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE'], dtype='<U8')"
      ]
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "source": [
    "test_set = ['I thorougly enjoyed this, 5 stars', 'bad book do not buy', 'horrible waste of time', 'piece of shit']\n",
    "new_test = vectorizer.transform(test_set)\n",
    "new_clf_lor.predict(new_test)"
   ]
  }
 ]
}